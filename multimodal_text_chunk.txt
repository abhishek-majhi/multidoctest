Multimodal learning refers to the process of learning representations from
different types of input modalities, such as image data, text or speech. Due to
methodological breakthroughs in the fields of Natural Language Processing
(NLP) as well as Computer Vision (CV) in recent years, multimodal models
have gained increasing attention as they are able to strengthen predictions
and better emulate the way humans learn. This chapter focuses on discussing
images and text as input data. The remainder of the chapter is structured as
follows:
The first part “Image2Text” discusses how transformer-based architectures
improve meaningful captioning for complex images using a new large scale,
richly annotated dataset COCO (Lin et al., 2014c; Cornia et al., 2020). While
looking at a photograph and describing it or parsing a complex scene and
describing its context is not a difficult task for humans, it appears to be much
more complex and challenging for computers. We start with focusing on images
as input modalities. In 2014 Microsoft COCO was developed with a primary goal
of advancing the state-of-the-art (SOTA) in object recognition by diving deeper
into a broader question of scene understanding (Lin et al., 2014c). “COCO” in
this case is the acronym for Common Objects in Context. It addresses three
core problems in scene understanding: object detection (non-iconic views),
segmentation, and captioning. While for tasks like machine translation and
language understanding in NLP, transformer-based architecture are already
widely used, the potential for applications in the multi-modal context has not
been fully covered yet. With the help of the MS COCO dataset, the transformerbased architecture “Meshed-Memory Transformer for Image Captioning” (M2
)
will be introduced, which was able to improve both image encoding and the
language generation steps (Cornia et al., 2020). The performance of M2 and
other different fully-attentive models will be compared on the MS COCO
dataset.
Next, in Text2Image, the idea of incorporating textual input in order to generate
visual representations is described. Current advancements in this field have been
made possible largely due to recent breakthroughs in NLP, which first allowed

for learning contextual representations of text. Transformer-like architectures
are being used to encode the input into embedding vectors, which are later
helpful in guiding the process of image generation. The chapter discusses the
development of the field in chronological order, looking into details of the most
recent milestones. Concepts such as generative adversarial networks (GAN),
variational auto-encoders (VAE), VAE with vector quantization (VQ-VAE),
diffusion, and autoregressive models are covered to provide the reader with a
better understanding of the roots of the current research and where it might be
heading. Some of the most outstanding outputs generated by state-of-the-art
works are also presented in the chapter.
The third part, “Images supporting Language Models”, deals with the integration of visual elements in pure textual language models. Distributional
semantic models such as Word2Vec and BERT assume that the meaning of
a given word or sentence can be understood by looking at how (in which
context) and when the word or the sentence appear in the text corpus, namely
from its “distribution” within the text. But this assumption has been historically questioned, because words and sentences must be grounded in other
perceptual dimensions in order to understand their meaning (see for example
the “symbol grounding problem”; Harnad, 1990). For these reasons, a broad
range of models has been developed with the aim to improve pure language
models, leveraging the addition of other perceptual dimensions, such as the
visual one. This subchapter focuses in particular on the integration of visual
elements (here: images) to support pure language models for various tasks
at the word-/token-level as well as on the sentence-level. The starting point
in this case is always a language model, into which visual representations
(extracted often with the help of large pools of images rom data sets like MS
COCO, see chapter “Img2Text” for further references) are to be “integrated”.
But how? There has been proposed a wide range of solutions: On one side
of the spectrum, textual elements and visual ones are learned separately and
then “combined” afterwards, whereas on the other side, the learning of textual
and visual features takes place simultaneously/jointly.
For example, Silberer and Lapata (2014) implement a model where a one-toone correspondence between textual and visual space is assumed. Text and
visual representations are passed to two separate unimodal encoders and both
outputs are then fed to a bimodal autoencoder. On the other side, Bordes
et al. (2020) propose a “text objective function” whose parameters are shared
with an additional “grounded objective function”. The training of the latter
takes place in what the authors called a “grounded space”, which allows to
avoid the one-to-one correspondence between textual and visual space. These
are just introductory examples and between these two approaches there are
many shades of gray (probably even more than fifty ..). These models exhibit
in many instances better performance than pure language models, but they
still struggle on some aspects, for example when they deal with abstract words
and sentences.

